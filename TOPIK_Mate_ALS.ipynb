{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pyspark의 ALS 활용하기\n",
        "### https://techblog-history-younghunjo1.tistory.com/161 참고"
      ],
      "metadata": {
        "id": "jgYC7uwBEGCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless"
      ],
      "metadata": {
        "id": "LgB8TeDk_htI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://downloads.apache.org/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "HEIiQtwo_2NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvf spark-3.2.4-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "a-t93o-WARKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "id": "UisCLiVy_3dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.4-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "3iMInY1x_plS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "_QAOnHcg_ww-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wO5jjCYF_yw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import random\n",
        "\n",
        "# Pysparkk Library #\n",
        "# SQL\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.functions import mean, col, split, regexp_extract, when, lit\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, IndexToString\n",
        "from pyspark.ml.feature import QuantileDiscretizer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
      ],
      "metadata": {
        "id": "MOPOj1GO-rIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 스파크 세션 만들기\n",
        "spark = SparkSession\\\n",
        "        .builder\\\n",
        "        .appName('TOPIK Mate ALS')\\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "AcEi60jO--XH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "eiHqp_FVG3Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용자 데이터\n",
        "users = pd.read_csv('/content/drive/MyDrive/캡스톤디자인_공유폴더/datas/contents/users_sample.csv')"
      ],
      "metadata": {
        "id": "Opg8kCOdDYvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users.sample(20)"
      ],
      "metadata": {
        "id": "ey7Yt0TwEm2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RLHHfvya8o8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문제 데이터\n",
        "problems = pd.read_csv('/content/drive/MyDrive/캡스톤디자인_공유폴더/datas/contents/questions.csv')\n",
        "problems.sample(20)"
      ],
      "metadata": {
        "id": "lUdJDplIEZ24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 답안과 합치기 - pandas.merge()\n",
        "\n",
        "solved = pd.merge(users, problems)\n",
        "solved.head()"
      ],
      "metadata": {
        "id": "kfgPRlh3FSgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del users\n",
        "del problems"
      ],
      "metadata": {
        "id": "aYSV1b50yp5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "solved['elapsed_time'] = pd.to_numeric(solved['elapsed_time'])"
      ],
      "metadata": {
        "id": "tbJKm4dq2Lrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "solved.sample(20)"
      ],
      "metadata": {
        "id": "WeTl65TDJ6Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문제 ID 별 분류\n",
        "questions = solved.groupby(solved.question_id)\n",
        "questions.size()"
      ],
      "metadata": {
        "id": "_bqt56XSFVHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del solved"
      ],
      "metadata": {
        "id": "me1CSMg0Sae4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = pd.DataFrame()\n",
        "\n",
        "# 그룹별 정/오답 그룹 정규 분포화\n",
        "idx = 0\n",
        "for key, group in questions:\n",
        "\n",
        "    # if (idx == 1):\n",
        "    #     break;\n",
        "\n",
        "    # print(f'[{key}] ============= ')\n",
        "\n",
        "    # group = questions.get_group(key)\n",
        "\n",
        "    # 정답 그룹\n",
        "    correct_group = group[group['user_answer'] == group['correct_answer']]\n",
        "    # 오답 그룹\n",
        "    wrong_group = group[group['user_answer'] != group['correct_answer']]\n",
        "\n",
        "    # print(f'group.shape: {group.shape}, correct_group.shape: {correct_group.shape}, wrong_group.shape: {wrong_group.shape}')\n",
        "\n",
        "    pivot = 0\n",
        "    # 정답 그룹 pivot 값\n",
        "    if correct_group.size > 0:\n",
        "\n",
        "        # N% 구간의 기준값 찾기\n",
        "        pivot = np.percentile(correct_group['elapsed_time'], 15)\n",
        "        try:\n",
        "            correct_group.loc[correct_group['elapsed_time'] <= pivot, 'label'] = 1\n",
        "            correct_group.loc[correct_group['elapsed_time'] > pivot, 'label'] = 2\n",
        "        except ValueError as e:\n",
        "            print('error: ', e)\n",
        "\n",
        "        result_df = pd.concat([result_df, correct_group], axis=0)\n",
        "\n",
        "    # print('correct_group: ', correct_group)\n",
        "\n",
        "    # labeled1 = correct_group\n",
        "    # labeled1['label'] = np.where(labeled1['elapsed_time'] <= pivot, 1, 2)\n",
        "\n",
        "\n",
        "    pivot2 = 0\n",
        "    # 오답 그룹 pivot 값\n",
        "    if wrong_group.size > 0:\n",
        "        pivot2 = np.percentile(wrong_group['elapsed_time'], 15)\n",
        "        try:\n",
        "            wrong_group.loc[wrong_group['elapsed_time'] <= pivot2, 'label'] = 4\n",
        "            wrong_group.loc[wrong_group['elapsed_time'] > pivot2, 'label'] = 5\n",
        "        except ValueError as e:\n",
        "            print('error: ', e)\n",
        "        result_df = pd.concat([result_df, wrong_group], axis=0)\n",
        "\n",
        "\n",
        "    # print('wrong_group: ', wrong_group)\n",
        "    # labeled2 = wrong_group\n",
        "    # labeled2['label'] = np.where(labeled2['elapsed_time'] <= pivot2, 5, 4)\n",
        "\n",
        "\n",
        "    # merge\n",
        "    # temp = pd.concat([labeled1, labeled2], axis=0)\n",
        "    # temp = pd.concat([correct_group, wrong_group], axis=0)\n",
        "\n",
        "    idx += 1\n"
      ],
      "metadata": {
        "id": "rQzjUFFeFwJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del questions"
      ],
      "metadata": {
        "id": "-nDfA0rO4GaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df[result_df['question_id'] != 'q1'].sample(20)"
      ],
      "metadata": {
        "id": "GRHH88cvHVkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ALS 추천 알고리즘\n",
        "from pyspark.ml.recommendation import ALS"
      ],
      "metadata": {
        "id": "24eF33H3F_Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 스키마 정의\n",
        "df_schema = StructType([StructField(\"user_id\", StringType(), True)\\\n",
        "                        ,StructField(\"solving_id\", StringType(), True)\\\n",
        "                        ,StructField(\"question_id\", StringType(), True)\\\n",
        "                        ,StructField(\"user_answer\", StringType(), True)\\\n",
        "                        ,StructField(\"bundle_id\", StringType(), True)\\\n",
        "                        ,StructField(\"explanation_id\", StringType(), True)\\\n",
        "                        ,StructField(\"correct_answer\", StringType(), True)\\\n",
        "                        ,StructField(\"part\", StringType(), True)\\\n",
        "                        ,StructField(\"tags\", StringType(), True)\\\n",
        "                        ,StructField(\"deployed_at\", StringType(), True)\\\n",
        "                        ,StructField(\"elapsed_time\", LongType(), True)\\\n",
        "                        ,StructField(\"label\", FloatType(), True)])\n",
        "\n",
        "# Pandas -> Spark 변환\n",
        "spark_df = spark.createDataFrame(result_df, schema=df_schema)\n",
        "display(spark_df)"
      ],
      "metadata": {
        "id": "Ztrf8-dpBCfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, IndexToString\n",
        "\n",
        "# 문자열인 user_id, question_id를 수치형 데이터로 바꾸기\n",
        "stringIndexer = StringIndexer(inputCols=['user_id', 'question_id'],\n",
        "                              outputCols=['user_id_num', 'question_id_num'],\n",
        "                              handleInvalid='error',\n",
        "                              stringOrderType='alphabetDesc')\n",
        "# stringIndexer = StringIndexer(inputCol='user_id', outputCol='user_id_num')\n",
        "stringIndexerModel = stringIndexer.fit(spark_df)\n",
        "encoded_df = stringIndexerModel.transform(spark_df)\n"
      ],
      "metadata": {
        "id": "7LseFzgALgjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_df.limit(5).toPandas()"
      ],
      "metadata": {
        "id": "pz_5QdsJNko8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습, 테스트 데이터 분리\n",
        "train, test = encoded_df.randomSplit([0.75, 0.25])"
      ],
      "metadata": {
        "id": "cusEH9o6KvFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rec = ALS(maxIter = 10,\n",
        "          regParam = 0.01,\n",
        "          userCol = 'user_id_num',\n",
        "          itemCol = 'question_id_num',\n",
        "          ratingCol = 'label',\n",
        "          nonnegative = True,\n",
        "          coldStartStrategy='drop')\n",
        "\n",
        "\n",
        "# ALS 모델 학습 -> dataframe을 넣어주기\n",
        "rec_model = rec.fit(train)"
      ],
      "metadata": {
        "id": "WIaiG9hJ7Wg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainsform을 이용해 예측 -> dataframe을 넣어주기\n",
        "pred_labels = rec_model.transform(test)\n",
        "pred_labels.limit(5)"
      ],
      "metadata": {
        "id": "lQyfO27AWYhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 실제 평점과 예측 평점 사이의 차이값으로 RMSE와 MAE값을 측정\n",
        "# Get metric for training\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "evaluator = RegressionEvaluator(labelCol='label',\n",
        "                                predictionCol='prediction',\n",
        "                                metricName='rmse')\n",
        "\n",
        "# evaluate 메소드에 예측값 담겨있는 dataframe 넣어주기\n",
        "rmse = evaluator.evaluate(pred_labels)\n",
        "\n",
        "mae_eval = RegressionEvaluator(labelCol='label',\n",
        "                               predictionCol='prediction',\n",
        "                               metricName='mae')\n",
        "\n",
        "mae = mae_eval.evaluate(pred_labels)\n",
        "\n",
        "print('RMSE: ', rmse)\n",
        "print('MAE: ', mae)"
      ],
      "metadata": {
        "id": "b6ZKfaul8AV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_df.head()"
      ],
      "metadata": {
        "id": "3G5O_Ujgfd2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 특정 유저에게 문제 추천해주기"
      ],
      "metadata": {
        "id": "g3DpXUKo4Qr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xr50Vavz4VL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pyspark Library #\n",
        "# SQL\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.functions import mean, col, split, regexp_extract, when, lit\n",
        "# ML\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, IndexToString\n",
        "from pyspark.ml.feature import QuantileDiscretizer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.recommendation import ALS"
      ],
      "metadata": {
        "id": "gyeO6-td4a-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_questions = encoded_df.select('question_id_num').distinct()\n",
        "encoded_df.toPandas()"
      ],
      "metadata": {
        "id": "gDZ2P0Ft47Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_questions(user_id, n):\n",
        "    \"\"\"\n",
        "    특정 유저에게 도움이 될 만한 n개의 문제를 추천\n",
        "    \"\"\"\n",
        "\n",
        "    # unique_questions 를 a라고 별칭\n",
        "    a = unique_questions.alias('a')\n",
        "\n",
        "    # 특정 유저가 푼 문제들만 담은 새로운 데이터프레임 생성\n",
        "    solved_questions = encoded_df.filter(encoded_df['user_id'] == user_id)\\\n",
        "                                    .select('question_id_num')\n",
        "\n",
        "    # 특정 유저가 푼 문제들을 b라고 별칭\n",
        "    b = solved_questions.alias('b')\n",
        "\n",
        "    # unique_questions를 기준으로 solved_questions를 조인시켜서\n",
        "    # 유저가 풀지 않은 문제들 파악 가능\n",
        "    total_questions = a.join(b, a['question_id_num'] == b['question_id_num'],\n",
        "                             how='left')\n",
        "\n",
        "    # b 데이터프레임의 question_id_num값이 결측치를 갖고 있는 행의\n",
        "    # a.question_id_num를 뽑아냄으로써 유저가 아직 보지 못한 문제들을 추출\n",
        "    remaining_questionis = total_questions\\\n",
        "                            .where(col('b.question_id_num').isNull())\\\n",
        "                            .select('a.question_id_num').distinct()\n",
        "\n",
        "    print(remaining_questionis.limit(5).toPandas())\n",
        "\n",
        "    # remaining_questions 데이터프레임에 특정 유저값을 동일하게 새로운 변수로 추가해주기\n",
        "    # remaining_questions = remaining_questions.withColumn('user_id', lit(int(user_id)))\n",
        "\n",
        "    # 위에서 만든 ALS 모델을 사용하여 추천 평점 예측 후 n개 만큼 view ->\n",
        "    recommender = rec_model.transform(remaining_questionis)\\\n",
        "                            .orderBy('prediction', ascending=False)\\\n",
        "                            .limit(n)\n",
        "\n",
        "    print(recommender)\n",
        "\n",
        "    # StringIndexer로 만든 것을 역으로\n",
        "    question_ids = IndexToString(inputCols = ['user_id_num', 'question_id_num'],\n",
        "                                 outputCols = ['user_id', 'question_id'],\n",
        "                                 labels=model.label)\n",
        "\n",
        "    # transform해서 문제id, 유저id를 숫자 -> 문자열로 변환\n",
        "    final_recommendations = question_ids.transform(recommender)\n",
        "\n",
        "    return final_recommendations.show(n, truncate=False)\n"
      ],
      "metadata": {
        "id": "rUcLteDD9IPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_questions('u1000', 5)"
      ],
      "metadata": {
        "id": "H14CZ6EdAXg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ZJyJmtSAifj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}